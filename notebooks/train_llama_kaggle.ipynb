{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA Training on Kaggle\n",
    "\n",
    "This notebook trains a LLaMA model from scratch on Kaggle's free GPU.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Create a new Kaggle notebook\n",
    "2. Enable GPU: Settings > Accelerator > GPU T4 x2\n",
    "3. Upload this repository as a dataset or clone from GitHub\n",
    "4. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if not uploaded as dataset)\n",
    "!git clone https://github.com/YOUR_USERNAME/llm-from-scratch.git\n",
    "%cd llm-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchinfo pytorch-lightning wandb datasets regex tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(\n",
    "    f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"No GPU\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "from scripts.config import Config\n",
    "\n",
    "# Configuration\n",
    "model_type = \"llama\"\n",
    "tokenizer_dir = \"./tokenizers\"\n",
    "num_samples = 100000\n",
    "\n",
    "# Load config\n",
    "config = Config.for_llama()\n",
    "\n",
    "# Create tokenizer directory\n",
    "Path(tokenizer_dir).mkdir(parents=True, exist_ok=True)\n",
    "tokenizer_path = Path(tokenizer_dir) / f\"{model_type}_tokenizer.json\"\n",
    "\n",
    "print(f\"Preparing tokenizer for {model_type}\")\n",
    "print(f\"Dataset: {config.data.dataset_name}\")\n",
    "print(f\"Vocab size: {config.data.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\n",
    "    config.data.dataset_name,\n",
    "    config.data.dataset_config,\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "# Limit samples\n",
    "dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "print(f\"Using {len(dataset)} samples for tokenizer training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tokenizer\n",
    "def text_iterator():\n",
    "    for item in dataset:\n",
    "        yield item[config.data.text_column]\n",
    "\n",
    "\n",
    "print(\"Training tokenizer...\")\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=config.data.vocab_size,\n",
    "    special_tokens=special_tokens,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(text_iterator(), trainer=trainer)\n",
    "tokenizer.save(str(tokenizer_path))\n",
    "print(f\"Tokenizer saved to: {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from scripts.config import Config\n",
    "from scripts.lightning_module import LlamaLightningModule\n",
    "from utils.training_pipeline import get_data_module\n",
    "\n",
    "# Training configuration\n",
    "run_name = f\"llama_kaggle_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "use_wandb = False  # Set to True if you want to use W&B\n",
    "\n",
    "# Create config\n",
    "config = Config.for_llama()\n",
    "config.run_name = run_name\n",
    "config.checkpoint_dir = checkpoint_dir\n",
    "config.tokenizer_dir = tokenizer_dir\n",
    "config.wandb.enabled = use_wandb\n",
    "\n",
    "# Kaggle-specific optimizations\n",
    "config.data.batch_size = 16  # Reduce batch size for memory\n",
    "config.training.num_epochs = 1  # Quick training for demo\n",
    "\n",
    "# Create directories\n",
    "Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config.validate()\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Run name: {run_name}\")\n",
    "print(f\"  Batch size: {config.data.batch_size}\")\n",
    "print(f\"  Model dim: {config.model.model_dim}\")\n",
    "print(f\"  Num layers: {config.model.num_layers}\")\n",
    "print(f\"  Max epochs: {config.training.num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Prepare data\n",
    "print(\"Preparing data...\")\n",
    "data_module = get_data_module(config)\n",
    "data_module.setup(stage=\"fit\")\n",
    "\n",
    "# Initialize model\n",
    "print(\"Initializing model...\")\n",
    "pl_module = LlamaLightningModule(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=config.training.early_stopping_patience,\n",
    "        mode=\"min\",\n",
    "        verbose=True,\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        dirpath=f\"{config.checkpoint_dir}/{config.run_name}\",\n",
    "        filename=\"best_model\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Setup logger (optional)\n",
    "if config.wandb.enabled:\n",
    "    logger = WandbLogger(\n",
    "        project=config.wandb.project,\n",
    "        entity=config.wandb.entity,\n",
    "        name=config.run_name,\n",
    "        log_model=config.wandb.log_model,\n",
    "    )\n",
    "    logger.experiment.config.update(config.to_dict())\n",
    "else:\n",
    "    logger = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=config.training.num_epochs,\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    gradient_clip_val=config.training.gradient_clip_val,\n",
    "    precision=config.training.precision,\n",
    "    val_check_interval=config.training.val_check_interval,\n",
    "    log_every_n_steps=50,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\")\n",
    "trainer.fit(pl_module, datamodule=data_module)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "checkpoint_path = f\"{config.checkpoint_dir}/{config.run_name}/best_model.ckpt\"\n",
    "print(f\"Best checkpoint saved at: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Load trained model for inference\n",
    "checkpoint_path = f\"{config.checkpoint_dir}/{config.run_name}/best_model.ckpt\"\n",
    "model = LlamaLightningModule.load_from_checkpoint(checkpoint_path, config=config)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "# Test generation\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "encoding = tokenizer.encode(prompt)\n",
    "input_ids = torch.tensor([encoding.ids]).cuda()\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"Generating...\")\n",
    "\n",
    "# Simple greedy generation\n",
    "with torch.no_grad():\n",
    "    for _ in range(50):\n",
    "        outputs = model.model(input_ids)\n",
    "        next_token = outputs[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == config.inference.eos_idx:\n",
    "            break\n",
    "\n",
    "# Decode\n",
    "generated_ids = input_ids[0].cpu().tolist()\n",
    "generated_text = tokenizer.decode(generated_ids)\n",
    "print(f\"\\nGenerated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Checkpoint\n",
    "\n",
    "Download the trained model checkpoint to use locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create archive for download\n",
    "!tar -czf llama_model.tar.gz checkpoints/ tokenizers/\n",
    "print(\"Model and tokenizer saved to llama_model.tar.gz\")\n",
    "print(\"Download this file from Kaggle Output section\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}