class Tokenizer:
    def __init__(self):
        super().__init__()

    def train(self, text, vocab_size):
        pass

    def encode(self, text):
        pass

    def decode(self, text):
        pass

    def _get_stats(self, ids):
        pass

    def _merge(self, ids, pair, idx):
        pass
